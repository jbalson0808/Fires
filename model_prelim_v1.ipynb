{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd07a8cd81a1699e7e1c05d771835f6490edbb9a81a456e457283de8180856282df",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mgrs\n",
    "import sqlite3\n",
    "import math\n",
    "import mgrs\n",
    "from collections import Counter\n",
    "from itertools import product\n",
    "import statistics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from scikitplot.metrics import plot_roc\n",
    "from scikitplot.metrics import plot_precision_recall\n",
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from numpy import argmax\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_test(X_tr, X_te, y_tr, y_te, class_weight=None, threshold=False):\n",
    "    \n",
    "    # Build and Plot PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    # pca.fit(X_tr.toarray())\n",
    "    # X_pca = pca.transform(X_tr.toarray())\n",
    "\n",
    "    pca.fit(X_tr)\n",
    "    X_pca = pca.transform(X_tr)\n",
    "\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_tr, cmap=plt.cm.prism, edgecolor='k', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # Build and fit the model\n",
    "    # if class_weight:\n",
    "    #     model = DecisionTreeClassifier(class_weight=class_weight)\n",
    "    # else:\n",
    "        # model = DecisionTreeClassifier()\n",
    "    # model = RandomForestClassifier()\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test the model\n",
    "    y_pred = model.predict(X_te)\n",
    "    print('Precision score %s' % precision_score(y_te, y_pred))\n",
    "    print('Recall score %s' % recall_score(y_te, y_pred))\n",
    "    print('F1-score score %s' % f1_score(y_te, y_pred))\n",
    "    print('Accuracy score %s' % accuracy_score(y_te, y_pred))\n",
    "    \n",
    "    y_score = model.predict_proba(X_te)\n",
    "    fpr0, tpr0, thresholds = roc_curve(y_te, y_score[:, 1])\n",
    "    roc_auc0 = auc(fpr0, tpr0)\n",
    "    \n",
    "    # Calculate the best threshold\n",
    "    best_threshold = None\n",
    "    if threshold:\n",
    "        J = tpr0 - fpr0\n",
    "        ix = argmax(J) # take the value which maximizes the J variable\n",
    "        best_threshold = thresholds[ix]\n",
    "        # adjust score according to threshold.\n",
    "        y_score = np.array([[1, y[1]] if y[0] >= best_threshold else [0, y[1]] for y in y_score])\n",
    "        \n",
    "    \n",
    "    # Plot metrics \n",
    "    plot_roc(y_te, y_score)\n",
    "    plt.show()\n",
    "    \n",
    "    plot_precision_recall(y_te, y_score)\n",
    "    plt.show()\n",
    "    \n",
    "    plot_cumulative_gain(y_te, y_score)\n",
    "    plt.show()\n",
    "    \n",
    "    plot_lift_curve(y_te, y_score)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print a classification report\n",
    "    print(classification_report(y_te,y_pred))\n",
    "    return roc_auc0,fpr0,tpr0, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_csv('socal_fires_weather_mgrs_lag.csv')\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "  'is_fire      ' + str(np.sum(data_clean['is_fire']))\n",
    ", '\\nis_fire_lag1 ' + str(np.sum(data_clean['is_fire_lag1']))\n",
    ", '\\nis_fire_lag2 ' + str(np.sum(data_clean['is_fire_lag2']))\n",
    ", '\\nis_fire_lag3 ' + str(np.sum(data_clean['is_fire_lag3']))\n",
    ", '\\nis_fire_lag4 ' + str(np.sum(data_clean['is_fire_lag4']))\n",
    ", '\\nis_fire_lag5 ' + str(np.sum(data_clean['is_fire_lag5']))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.sort_values(by=['mgrs_10km','date'])\n",
    "data_clean.reset_index(drop=True,inplace=True)\n",
    "data_clean.drop(['mgrs_10km','date'],axis=1,inplace=True)\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[data_clean['is_fire']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean[17475:17483]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 0.33\n",
    "stratified_sample, _ = train_test_split(data_clean,test_size = (1-sample_size), stratify=data_clean[['is_fire']], random_state = 8)\n",
    "# print(stratified_sample)\n",
    "\n",
    "stratified_sample = pd.DataFrame(stratified_sample)\n",
    "# stratified_sample.drop(['mgrs_100km'], axis=1, inplace=True)\n",
    "stratified_sample.reset_index(drop=True, inplace=True)\n",
    "stratified_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(stratified_sample.is_fire) / len(stratified_sample))\n",
    "print(np.sum(data_clean.is_fire) / len(data_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stratified_sample.copy()\n",
    "y = X['is_fire']\n",
    "yLag1 = X['is_fire_lag1']\n",
    "yLag2 = X['is_fire_lag2']\n",
    "yLag3 = X['is_fire_lag3']\n",
    "yLag4 = X['is_fire_lag4']\n",
    "yLag5 = X['is_fire_lag5']\n",
    "X.drop(['is_fire','is_fire_lag1','is_fire_lag2','is_fire_lag3','is_fire_lag4','is_fire_lag5'], axis = 1, inplace = True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,yLag1,test_size = 0.33, random_state = 8)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training target statistics: {Counter(y_train)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_imb,fpr_imb,tpr_imb, _ = build_and_test(np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# over_sampler = RandomOverSampler(random_state=42)\n",
    "# X_res, y_res = over_sampler.fit_resample(X_train, y_train)\n",
    "# print(f\"Training target statistics: {Counter(y_res)}\")\n",
    "# print(f\"Testing target statistics: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_ros,fpr_ros,tpr_ros, _ = build_and_test(X_res, X_test, y_res, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "under_sampler = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = under_sampler.fit_resample(X_train, y_train)\n",
    "print(f\"Training target statistics: {Counter(y_res)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_rus,fpr_rus,tpr_rus , _ = build_and_test(X_res, X_test, y_res, y_test)"
   ]
  }
 ]
}